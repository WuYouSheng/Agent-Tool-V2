#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import base64
import hashlib
import time
import threading
from datetime import datetime
from scapy.all import *
from collections import defaultdict, deque
import queue


class FragmentOptimizedComparator:
    def __init__(self):
        # ç¶²è·¯æ¶æ§‹é…ç½®
        self.source_ip = "220.133.19.199"
        self.source_port = 5006
        self.local_ip = "140.115.52.14"
        self.internal_ip = "10.52.52.96"
        self.internal_port = 20
        self.target_ip = "10.52.52.100"
        self.target_port = 9999

        # çµ±è¨ˆè¨ˆæ•¸å™¨
        self.total_incoming_captured = 0
        self.total_embedded_captured = 0
        self.total_fragments_received = 0
        self.total_complete_packets = 0
        self.total_restored_packets = 0

        # åˆ†ç‰‡è™•ç†å„ªåŒ–
        self.fragment_buffer = defaultdict(dict)
        self.fragment_timeouts = defaultdict(float)
        self.fragment_timeout = 30.0  # 30ç§’è¶…æ™‚
        self.complete_fragments = []
        self.incomplete_fragments = defaultdict(set)

        # å°åŒ…å„²å­˜
        self.incoming_buffer = deque(maxlen=5000)
        self.restored_packets = []

        # åŒ¹é…æ§åˆ¶
        self.matched_incoming_ids = set()
        self.matched_restored_ids = set()
        self.unique_matches = []

        # åŒæ­¥æ§åˆ¶
        self.capture_start_time = None
        self.time_window = 15.0  # å¢åŠ æ™‚é–“çª—å£
        self.is_capturing = False

        # éšŠåˆ—
        self.incoming_queue = queue.Queue()
        self.restored_queue = queue.Queue()

        # åˆ†ç‰‡çµ±è¨ˆ
        self.fragment_stats = {
            'received_fragments': 0,
            'complete_groups': 0,
            'incomplete_groups': 0,
            'timeout_groups': 0,
            'successful_reassemblies': 0,
            'failed_reassemblies': 0
        }

    def print_network_config(self):
        """é¡¯ç¤ºç¶²è·¯é…ç½®"""
        print("ğŸ§© åˆ†ç‰‡å„ªåŒ–å°åŒ…æ¯”è¼ƒå·¥å…· v5.0")
        print("=" * 60)
        print(f"ğŸ“¥ ä¾†æºç«¯é»:     {self.source_ip}:{self.source_port}")
        print(f"ğŸ–¥ï¸  æœ¬æ©ŸIP:       {self.local_ip}")
        print(f"ğŸ“¤ å…§ç¶²ç™¼é€ç«¯é»: {self.internal_ip}:{self.internal_port}")
        print(f"ğŸ¯ ç›®æ¨™ç«¯é»:     {self.target_ip}:{self.target_port}")
        print(f"â° æ™‚é–“çª—å£:     {self.time_window} ç§’")
        print(f"ğŸ§© åˆ†ç‰‡è¶…æ™‚:     {self.fragment_timeout} ç§’")
        print("")
        print("ğŸ”§ åˆ†ç‰‡å„ªåŒ–åŠŸèƒ½:")
        print("   1. æ™ºèƒ½åˆ†ç‰‡è¿½è¹¤ - è©³ç´°ç›£æ§åˆ†ç‰‡ç‹€æ…‹")
        print("   2. è¶…æ™‚æ¸…ç†æ©Ÿåˆ¶ - è‡ªå‹•æ¸…ç†éæœŸåˆ†ç‰‡")
        print("   3. é‡çµ„ç‹€æ…‹è¨ºæ–· - è­˜åˆ¥é‡çµ„å¤±æ•—åŸå› ")
        print("   4. å¢å¼·å®¹éŒ¯èƒ½åŠ› - æ›´å¥½çš„éŒ¯èª¤è™•ç†")

    def start_synchronized_capture(self, duration=60):
        """é–‹å§‹åŒæ­¥æ•ç²"""
        print(f"\nğŸš€ é–‹å§‹åˆ†ç‰‡å„ªåŒ–åŒæ­¥æ•ç² ({duration}ç§’)")
        print("=" * 60)

        # é‡ç½®æ‰€æœ‰è¨ˆæ•¸å™¨
        self._reset_counters()

        # è¨­å®šæ•ç²é–‹å§‹æ™‚é–“
        self.capture_start_time = time.time()
        self.is_capturing = True

        # å•Ÿå‹•åˆ†ç‰‡æ¸…ç†ç·šç¨‹
        cleanup_thread = threading.Thread(target=self._fragment_cleanup_worker, daemon=True)
        cleanup_thread.start()

        # å•Ÿå‹•åŒ¹é…è™•ç†ç·šç¨‹
        matching_thread = threading.Thread(target=self._fragment_aware_matching, daemon=True)
        matching_thread.start()

        # å•Ÿå‹•å…©å€‹æ•ç²ç·šç¨‹
        incoming_thread = threading.Thread(
            target=self._capture_incoming_packets,
            args=(duration,),
            daemon=True
        )

        outgoing_thread = threading.Thread(
            target=self._capture_outgoing_packets_optimized,
            args=(duration,),
            daemon=True
        )

        print(f"â° æ•ç²é–‹å§‹æ™‚é–“: {datetime.fromtimestamp(self.capture_start_time)}")

        # åŒæ™‚å•Ÿå‹•
        incoming_thread.start()
        outgoing_thread.start()

        # ç­‰å¾…å®Œæˆ
        incoming_thread.join()
        outgoing_thread.join()

        self.is_capturing = False

        # æœ€å¾Œçš„åˆ†ç‰‡æ¸…ç†å’Œçµ±è¨ˆ
        self._final_fragment_cleanup()

        # ç­‰å¾…æœ€å¾Œçš„åŒ¹é…è™•ç†
        time.sleep(3)

        print(f"\nâœ… åˆ†ç‰‡å„ªåŒ–æ•ç²å®Œæˆ")
        return self._comprehensive_analysis()

    def _reset_counters(self):
        """é‡ç½®æ‰€æœ‰è¨ˆæ•¸å™¨"""
        self.total_incoming_captured = 0
        self.total_embedded_captured = 0
        self.total_fragments_received = 0
        self.total_complete_packets = 0
        self.total_restored_packets = 0

        self.fragment_buffer.clear()
        self.fragment_timeouts.clear()
        self.complete_fragments.clear()
        self.incomplete_fragments.clear()

        self.fragment_stats = {
            'received_fragments': 0,
            'complete_groups': 0,
            'incomplete_groups': 0,
            'timeout_groups': 0,
            'successful_reassemblies': 0,
            'failed_reassemblies': 0
        }

    def _capture_incoming_packets(self, duration):
        """æ•ç²ä¾†æºå°åŒ…"""
        print(f"ğŸ“¥ å•Ÿå‹•ä¾†æºå°åŒ…æ•ç²...")

        def packet_handler(packet):
            if (TCP in packet and
                    packet[IP].src == self.source_ip and
                    packet[TCP].sport == self.source_port and
                    packet[IP].dst == self.local_ip):

                self.total_incoming_captured += 1
                capture_time = time.time()
                relative_time = capture_time - self.capture_start_time

                packet_bytes = bytes(packet)
                packet_info = {
                    'id': f"incoming_{self.total_incoming_captured}",
                    'timestamp': capture_time,
                    'relative_time': relative_time,
                    'packet': packet,
                    'size': len(packet_bytes),
                    'full_hash': hashlib.sha256(packet_bytes).hexdigest(),
                    'ip_hash': hashlib.sha256(bytes(packet[IP])).hexdigest() if IP in packet else None,
                    'content_signature': self._create_content_signature(packet),
                    'layers': self._get_packet_layers(packet),
                    'sequence_number': self.total_incoming_captured
                }

                self.incoming_buffer.append(packet_info)
                self.incoming_queue.put(packet_info)

                if self.total_incoming_captured % 100 == 0:
                    print(f"ğŸ“¦ ä¾†æºå°åŒ…: {self.total_incoming_captured} å€‹ (æ™‚é–“: +{relative_time:.1f}s)")

        try:
            filter_expr = f"tcp and src host {self.source_ip} and src port {self.source_port} and dst host {self.local_ip}"
            sniff(filter=filter_expr, prn=packet_handler, timeout=duration)
            print(f"âœ… ä¾†æºå°åŒ…æ•ç²å®Œæˆ: {self.total_incoming_captured} å€‹")

        except Exception as e:
            print(f"âŒ ä¾†æºå°åŒ…æ•ç²å¤±æ•—: {e}")

    def _capture_outgoing_packets_optimized(self, duration):
        """å„ªåŒ–çš„åµŒå…¥å°åŒ…æ•ç²"""
        print(f"ğŸ“¤ å•Ÿå‹•åˆ†ç‰‡å„ªåŒ–åµŒå…¥å°åŒ…æ•ç²...")

        def packet_handler(packet):
            if (TCP in packet and
                    packet[IP].src == self.internal_ip and
                    packet[TCP].sport == self.internal_port and
                    packet[IP].dst == self.target_ip and
                    packet[TCP].dport == self.target_port and
                    Raw in packet):

                self.total_embedded_captured += 1
                capture_time = time.time()
                relative_time = capture_time - self.capture_start_time

                try:
                    payload = packet[Raw].load.decode('utf-8')
                    embedded_data = json.loads(payload)

                    if "fragment_info" in embedded_data:
                        self.total_fragments_received += 1
                        self.fragment_stats['received_fragments'] += 1

                        # è™•ç†åˆ†ç‰‡
                        self._handle_fragment_optimized(embedded_data, capture_time, relative_time)

                        if self.total_fragments_received % 50 == 0:
                            print(
                                f"ğŸ§© åˆ†ç‰‡å°åŒ…: {self.total_fragments_received} å€‹ (å®Œæ•´çµ„: {self.fragment_stats['complete_groups']})")

                    elif "metadata" in embedded_data and "original_packet" in embedded_data:
                        self.total_complete_packets += 1

                        # è™•ç†å®Œæ•´å°åŒ…
                        restored_info = self._process_complete_packet(embedded_data, capture_time, relative_time)
                        if restored_info:
                            self.restored_queue.put(restored_info)

                except Exception as e:
                    # è©³ç´°è¨˜éŒ„è§£æéŒ¯èª¤
                    if self.total_embedded_captured % 100 == 0:
                        print(f"âš ï¸  è§£æéŒ¯èª¤: {str(e)[:50]}...")

        try:
            filter_expr = f"tcp and src host {self.internal_ip} and src port {self.internal_port} and dst host {self.target_ip} and dst port {self.target_port}"
            sniff(filter=filter_expr, prn=packet_handler, timeout=duration)
            print(f"âœ… åµŒå…¥å°åŒ…æ•ç²å®Œæˆ: {self.total_embedded_captured} å€‹")
            print(f"ğŸ§© åˆ†ç‰‡å°åŒ…: {self.total_fragments_received} å€‹")
            print(f"ğŸ“‹ å®Œæ•´å°åŒ…: {self.total_complete_packets} å€‹")
            print(f"ğŸ”„ é‚„åŸå°åŒ…: {self.total_restored_packets} å€‹")

        except Exception as e:
            print(f"âŒ åµŒå…¥å°åŒ…æ•ç²å¤±æ•—: {e}")

    def _handle_fragment_optimized(self, embedded_data, capture_time, relative_time):
        """å„ªåŒ–çš„åˆ†ç‰‡è™•ç†"""
        try:
            fragment_info = embedded_data["fragment_info"]
            fragment_uuid = fragment_info["fragment_uuid"]
            fragment_index = fragment_info["fragment_index"]
            total_fragments = fragment_info["total_fragments"]

            # è¨˜éŒ„åˆ†ç‰‡æ™‚é–“
            self.fragment_timeouts[fragment_uuid] = capture_time

            # å„²å­˜åˆ†ç‰‡
            self.fragment_buffer[fragment_uuid][fragment_index] = {
                'data': embedded_data,
                'timestamp': capture_time,
                'relative_time': relative_time
            }

            # æ›´æ–°ä¸å®Œæ•´åˆ†ç‰‡è¿½è¹¤
            if fragment_uuid not in self.incomplete_fragments:
                self.incomplete_fragments[fragment_uuid] = set()
            self.incomplete_fragments[fragment_uuid].add(fragment_index)

            # æª¢æŸ¥æ˜¯å¦å®Œæ•´
            if len(self.fragment_buffer[fragment_uuid]) == total_fragments:
                print(f"ğŸ§© åˆ†ç‰‡çµ„å®Œæ•´: {fragment_uuid[:8]}... ({total_fragments}å€‹åˆ†ç‰‡)")

                # å˜—è©¦é‡çµ„
                success = self._reassemble_fragments_optimized(fragment_uuid, capture_time, relative_time)

                if success:
                    self.fragment_stats['complete_groups'] += 1
                    self.fragment_stats['successful_reassemblies'] += 1

                    # å¾ä¸å®Œæ•´åˆ—è¡¨ä¸­ç§»é™¤
                    if fragment_uuid in self.incomplete_fragments:
                        del self.incomplete_fragments[fragment_uuid]

                else:
                    self.fragment_stats['failed_reassemblies'] += 1
                    print(f"âŒ åˆ†ç‰‡é‡çµ„å¤±æ•—: {fragment_uuid[:8]}...")

            else:
                # é¡¯ç¤ºåˆ†ç‰‡é€²åº¦
                received = len(self.fragment_buffer[fragment_uuid])
                if received % 5 == 0 or received == total_fragments - 1:
                    print(f"ğŸ§© åˆ†ç‰‡é€²åº¦: {fragment_uuid[:8]}... [{received}/{total_fragments}]")

        except Exception as e:
            print(f"âŒ åˆ†ç‰‡è™•ç†éŒ¯èª¤: {e}")

    def _reassemble_fragments_optimized(self, fragment_uuid, capture_time, relative_time):
        """å„ªåŒ–çš„åˆ†ç‰‡é‡çµ„"""
        try:
            fragments = self.fragment_buffer[fragment_uuid]

            # æŒ‰ç´¢å¼•æ’åº
            sorted_fragments = sorted(fragments.items())

            # æª¢æŸ¥åˆ†ç‰‡å®Œæ•´æ€§
            expected_indices = set(range(len(sorted_fragments)))
            actual_indices = set(fragments.keys())

            if expected_indices != actual_indices:
                print(f"âš ï¸  åˆ†ç‰‡ç´¢å¼•ä¸å®Œæ•´: æœŸæœ›{expected_indices}, å¯¦éš›{actual_indices}")
                return False

            # é‡çµ„æ•¸æ“š
            reassembled_data = b""
            for _, fragment_container in sorted_fragments:
                try:
                    fragment_data = fragment_container['data']
                    fragment_bytes = base64.b64decode(fragment_data["data"])
                    reassembled_data += fragment_bytes
                except Exception as e:
                    print(f"âŒ åˆ†ç‰‡æ•¸æ“šè§£ç¢¼å¤±æ•—: {e}")
                    return False

            # è§£æå®Œæ•´æ•¸æ“š
            try:
                complete_data = json.loads(reassembled_data.decode('utf-8'))
            except Exception as e:
                print(f"âŒ é‡çµ„æ•¸æ“šJSONè§£æå¤±æ•—: {e}")
                return False

            # è™•ç†é‡çµ„å¾Œçš„å®Œæ•´å°åŒ…
            restored_info = self._process_complete_packet(complete_data, capture_time, relative_time)

            if restored_info:
                restored_info['is_reassembled'] = True
                restored_info['fragment_count'] = len(sorted_fragments)
                restored_info['fragment_uuid'] = fragment_uuid

                self.restored_queue.put(restored_info)

                print(f"âœ… åˆ†ç‰‡é‡çµ„æˆåŠŸ: {fragment_uuid[:8]}... â†’ é‚„åŸå°åŒ…")

                # æ¸…ç†åˆ†ç‰‡ç·©å­˜
                del self.fragment_buffer[fragment_uuid]
                if fragment_uuid in self.fragment_timeouts:
                    del self.fragment_timeouts[fragment_uuid]

                return True
            else:
                print(f"âŒ é‡çµ„å¾Œå°åŒ…è™•ç†å¤±æ•—: {fragment_uuid[:8]}...")
                return False

        except Exception as e:
            print(f"âŒ åˆ†ç‰‡é‡çµ„éŒ¯èª¤: {e}")
            return False

    def _process_complete_packet(self, embedded_data, capture_time, relative_time):
        """è™•ç†å®Œæ•´å°åŒ…"""
        try:
            self.total_restored_packets += 1

            metadata = embedded_data.get("metadata", {})
            original_packet_data = embedded_data.get("original_packet", {})

            if not original_packet_data.get("data"):
                print(f"âš ï¸  å°åŒ…æ•¸æ“šç¼ºå¤±")
                return None

            # è§£ç¢¼åŸå§‹å°åŒ…
            original_bytes = base64.b64decode(original_packet_data["data"])

            # æ™ºèƒ½é‚„åŸ
            layers = original_packet_data.get('layers', [])
            if "IP" in layers:
                restored_packet = IP(original_bytes)
            else:
                restored_packet = Ether(original_bytes)

            # è¨ˆç®—ç‰¹å¾µ
            restored_bytes = bytes(restored_packet)

            restored_info = {
                'id': f"restored_{self.total_restored_packets}",
                'timestamp': capture_time,
                'relative_time': relative_time,
                'packet': restored_packet,
                'size': len(restored_bytes),
                'full_hash': hashlib.sha256(restored_bytes).hexdigest(),
                'ip_hash': hashlib.sha256(bytes(restored_packet[IP])).hexdigest() if IP in restored_packet else None,
                'content_signature': self._create_content_signature(restored_packet),
                'layers': self._get_packet_layers(restored_packet),
                'metadata': metadata,
                'original_hash': original_packet_data.get('original_hash', ''),
                'restoration_method': 'ip_layer' if "IP" in layers else 'ether_layer',
                'sequence_number': self.total_restored_packets
            }

            self.restored_packets.append(restored_info)
            return restored_info

        except Exception as e:
            print(f"âŒ å®Œæ•´å°åŒ…è™•ç†éŒ¯èª¤: {e}")
            return None

    def _fragment_cleanup_worker(self):
        """åˆ†ç‰‡æ¸…ç†å·¥ä½œç·šç¨‹"""
        while self.is_capturing:
            try:
                current_time = time.time()
                expired_fragments = []

                # æ‰¾å‡ºéæœŸçš„åˆ†ç‰‡
                for fragment_uuid, last_update in self.fragment_timeouts.items():
                    if current_time - last_update > self.fragment_timeout:
                        expired_fragments.append(fragment_uuid)

                # æ¸…ç†éæœŸåˆ†ç‰‡
                for fragment_uuid in expired_fragments:
                    fragments_count = len(self.fragment_buffer.get(fragment_uuid, {}))
                    print(f"ğŸ§¹ æ¸…ç†éæœŸåˆ†ç‰‡: {fragment_uuid[:8]}... ({fragments_count}å€‹åˆ†ç‰‡)")

                    if fragment_uuid in self.fragment_buffer:
                        del self.fragment_buffer[fragment_uuid]
                    if fragment_uuid in self.fragment_timeouts:
                        del self.fragment_timeouts[fragment_uuid]
                    if fragment_uuid in self.incomplete_fragments:
                        del self.incomplete_fragments[fragment_uuid]

                    self.fragment_stats['timeout_groups'] += 1

                # é¡¯ç¤ºåˆ†ç‰‡ç‹€æ…‹
                if len(self.fragment_buffer) > 0:
                    active_groups = len(self.fragment_buffer)
                    if active_groups % 10 == 0:
                        print(f"ğŸ§© æ´»èºåˆ†ç‰‡çµ„: {active_groups} å€‹")

                time.sleep(5)  # æ¯5ç§’æª¢æŸ¥ä¸€æ¬¡

            except Exception as e:
                print(f"åˆ†ç‰‡æ¸…ç†éŒ¯èª¤: {e}")
                time.sleep(5)

    def _final_fragment_cleanup(self):
        """æœ€çµ‚åˆ†ç‰‡æ¸…ç†å’Œçµ±è¨ˆ"""
        print(f"\nğŸ§¹ åŸ·è¡Œæœ€çµ‚åˆ†ç‰‡æ¸…ç†...")

        # çµ±è¨ˆå‰©é¤˜åˆ†ç‰‡
        remaining_groups = len(self.fragment_buffer)
        incomplete_groups = len(self.incomplete_fragments)

        if remaining_groups > 0:
            print(f"âš ï¸  å‰©é¤˜æœªå®Œæˆåˆ†ç‰‡çµ„: {remaining_groups} å€‹")

            # é¡¯ç¤ºå‰5å€‹æœªå®Œæˆçš„åˆ†ç‰‡çµ„è©³æƒ…
            for i, (fragment_uuid, fragments) in enumerate(list(self.fragment_buffer.items())[:5]):
                total_expected = len(fragments)
                received_indices = sorted(fragments.keys())
                print(f"   {i + 1}. {fragment_uuid[:8]}...: {len(fragments)} å€‹åˆ†ç‰‡ {received_indices}")

        self.fragment_stats['incomplete_groups'] = remaining_groups

    def _fragment_aware_matching(self):
        """åˆ†ç‰‡æ„ŸçŸ¥çš„åŒ¹é…è™•ç†"""
        print(f"ğŸ”„ å•Ÿå‹•åˆ†ç‰‡æ„ŸçŸ¥åŒ¹é…å¼•æ“...")

        pending_incoming = []
        pending_restored = []

        while self.is_capturing or not self.incoming_queue.empty() or not self.restored_queue.empty():
            try:
                # æ”¶é›†å°åŒ…
                while not self.incoming_queue.empty():
                    incoming_info = self.incoming_queue.get_nowait()
                    if incoming_info['id'] not in self.matched_incoming_ids:
                        pending_incoming.append(incoming_info)

                while not self.restored_queue.empty():
                    restored_info = self.restored_queue.get_nowait()
                    if restored_info['id'] not in self.matched_restored_ids:
                        pending_restored.append(restored_info)

                # åŸ·è¡ŒåŒ¹é…
                new_matches = self._enhanced_packet_matching(pending_incoming, pending_restored)
                self.unique_matches.extend(new_matches)

                if len(self.unique_matches) % 25 == 0 and len(self.unique_matches) > 0:
                    print(f"ğŸ¯ å”¯ä¸€åŒ¹é…: {len(self.unique_matches)} å€‹é…å°")

                # æ¸…ç†éæœŸå°åŒ…
                current_time = time.time()
                pending_incoming = [p for p in pending_incoming
                                    if (current_time - p['timestamp']) < self.time_window
                                    and p['id'] not in self.matched_incoming_ids]
                pending_restored = [p for p in pending_restored
                                    if (current_time - p['timestamp']) < self.time_window
                                    and p['id'] not in self.matched_restored_ids]

                time.sleep(0.2)

            except Exception as e:
                print(f"åŒ¹é…å¼•æ“éŒ¯èª¤: {e}")
                time.sleep(0.2)

    def _enhanced_packet_matching(self, incoming_list, restored_list):
        """å¢å¼·çš„å°åŒ…åŒ¹é…"""
        matches = []

        for restored_info in restored_list:
            if restored_info['id'] in self.matched_restored_ids:
                continue

            best_match = None
            best_score = 0
            best_incoming_info = None

            for incoming_info in incoming_list:
                if incoming_info['id'] in self.matched_incoming_ids:
                    continue

                # è¨ˆç®—åŒ¹é…åˆ†æ•¸
                score = self._calculate_enhanced_match_score(incoming_info, restored_info)

                if score > best_score and score > 0.5:  # é™ä½é–¾å€¼
                    best_match = incoming_info
                    best_score = score
                    best_incoming_info = incoming_info

            if best_match and best_incoming_info:
                matches.append({
                    'incoming': best_incoming_info,
                    'restored': restored_info,
                    'score': best_score,
                    'match_time': time.time(),
                    'time_diff': abs(restored_info['relative_time'] - best_incoming_info['relative_time']),
                    'match_type': self._determine_match_type(best_incoming_info, restored_info, best_score),
                    'is_reassembled': restored_info.get('is_reassembled', False)
                })

                self.matched_incoming_ids.add(best_incoming_info['id'])
                self.matched_restored_ids.add(restored_info['id'])

        return matches

    def _calculate_enhanced_match_score(self, incoming_info, restored_info):
        """å¢å¼·çš„åŒ¹é…åˆ†æ•¸è¨ˆç®—"""
        score = 0.0

        # 1. é›œæ¹ŠåŒ¹é…
        if incoming_info['full_hash'] == restored_info['full_hash']:
            score += 1.0
        elif incoming_info['ip_hash'] and restored_info['ip_hash'] and incoming_info['ip_hash'] == restored_info[
            'ip_hash']:
            score += 0.8

        # 2. å…§å®¹ç‰¹å¾µåŒ¹é…
        if incoming_info['content_signature'] == restored_info['content_signature']:
            score += 0.7

        # 3. å¤§å°ç›¸ä¼¼æ€§ (è€ƒæ…®IP vs Etherå·®ç•°)
        size_diff = abs(incoming_info['size'] - restored_info['size'])
        if size_diff <= 26:  # IP header vs Ether header å·®ç•°
            score += 0.6
        elif size_diff <= 50:
            score += 0.4
        elif size_diff <= 100:
            score += 0.2

        # 4. æ™‚é–“æ¥è¿‘æ€§
        time_diff = abs(restored_info['relative_time'] - incoming_info['relative_time'])
        if time_diff < self.time_window:
            time_score = max(0, 1.0 - (time_diff / self.time_window))
            score += time_score * 0.5

        # 5. å±¤ç´šåŒ¹é…
        incoming_layers = set(incoming_info['layers'])
        restored_layers = set(restored_info['layers'])
        common_layers = incoming_layers & restored_layers
        if common_layers:
            layer_score = len(common_layers) / max(len(incoming_layers), len(restored_layers))
            score += layer_score * 0.3

        return min(score, 1.0)

    def _determine_match_type(self, incoming_info, restored_info, score):
        """ç¢ºå®šåŒ¹é…é¡å‹"""
        if incoming_info['full_hash'] == restored_info['full_hash']:
            return "å®Œæ•´é›œæ¹ŠåŒ¹é…"
        elif incoming_info['ip_hash'] == restored_info['ip_hash']:
            return "IPå±¤é›œæ¹ŠåŒ¹é…"
        elif incoming_info['content_signature'] == restored_info['content_signature']:
            return "å…§å®¹ç‰¹å¾µåŒ¹é…"
        elif score >= 0.8:
            return "é«˜ç›¸ä¼¼åº¦åŒ¹é…"
        elif score >= 0.6:
            return "ä¸­ç­‰ç›¸ä¼¼åº¦åŒ¹é…"
        else:
            return "ä½ç›¸ä¼¼åº¦åŒ¹é…"

    def _create_content_signature(self, packet):
        """å‰µå»ºå°åŒ…å…§å®¹ç‰¹å¾µ"""
        signature = []

        if IP in packet:
            signature.extend([packet[IP].src, packet[IP].dst, str(packet[IP].proto)])

        if TCP in packet:
            signature.extend([str(packet[TCP].sport), str(packet[TCP].dport)])

        if Raw in packet:
            payload = packet[Raw].load
            payload_hash = hashlib.md5(payload[:50]).hexdigest()[:8]
            signature.append(payload_hash)

        return "|".join(signature)

    def _get_packet_layers(self, packet):
        """ç²å–å°åŒ…å±¤ç´š"""
        layers = []
        current = packet
        while current:
            layer_name = current.__class__.__name__
            layers.append(layer_name)
            if hasattr(current, 'payload') and current.payload:
                current = current.payload
            else:
                break
        return layers

    def _comprehensive_analysis(self):
        """å…¨é¢åˆ†æçµæœ"""
        print(f"\nğŸ§© å…¨é¢åˆ†ç‰‡å„ªåŒ–åˆ†æ")
        print("=" * 80)

        print(f"ğŸ“Š å°åŒ…çµ±è¨ˆ:")
        print(f"   å¯¦éš›æ•ç²ä¾†æºå°åŒ…: {self.total_incoming_captured} å€‹")
        print(f"   åµŒå…¥å°åŒ…ç¸½æ•¸: {self.total_embedded_captured} å€‹")
        print(f"   åˆ†ç‰‡å°åŒ…ç¸½æ•¸: {self.total_fragments_received} å€‹")
        print(f"   å®Œæ•´å°åŒ…ç¸½æ•¸: {self.total_complete_packets} å€‹")
        print(f"   æˆåŠŸé‚„åŸå°åŒ…: {self.total_restored_packets} å€‹")
        print(f"   å”¯ä¸€æˆåŠŸåŒ¹é…: {len(self.unique_matches)} å€‹")

        # è¨ˆç®—å„ç¨®æ•ˆç‡
        if self.total_incoming_captured > 0:
            match_rate = (len(self.unique_matches) / self.total_incoming_captured) * 100
        else:
            match_rate = 0

        if self.total_embedded_captured > 0:
            restoration_rate = (self.total_restored_packets / self.total_embedded_captured) * 100
        else:
            restoration_rate = 0

        if self.total_restored_packets > 0:
            matching_efficiency = (len(self.unique_matches) / self.total_restored_packets) * 100
        else:
            matching_efficiency = 0

        print(f"   çœŸå¯¦æˆåŠŸç‡: {match_rate:.1f}%")
        print(f"   å°åŒ…é‚„åŸç‡: {restoration_rate:.1f}%")
        print(f"   åŒ¹é…æ•ˆç‡: {matching_efficiency:.1f}%")

        # åˆ†ç‰‡è©³ç´°çµ±è¨ˆ
        print(f"\nğŸ§© åˆ†ç‰‡è™•ç†è©³ç´°çµ±è¨ˆ:")
        print(f"   æ¥æ”¶åˆ†ç‰‡ç¸½æ•¸: {self.fragment_stats['received_fragments']} å€‹")
        print(f"   å®Œæ•´åˆ†ç‰‡çµ„: {self.fragment_stats['complete_groups']} å€‹")
        print(f"   ä¸å®Œæ•´åˆ†ç‰‡çµ„: {self.fragment_stats['incomplete_groups']} å€‹")
        print(f"   è¶…æ™‚åˆ†ç‰‡çµ„: {self.fragment_stats['timeout_groups']} å€‹")
        print(f"   æˆåŠŸé‡çµ„: {self.fragment_stats['successful_reassemblies']} å€‹")
        print(f"   é‡çµ„å¤±æ•—: {self.fragment_stats['failed_reassemblies']} å€‹")

        # è¨ˆç®—åˆ†ç‰‡æ•ˆç‡
        total_fragment_groups = (self.fragment_stats['complete_groups'] +
                                 self.fragment_stats['incomplete_groups'] +
                                 self.fragment_stats['timeout_groups'])

        if total_fragment_groups > 0:
            fragment_success_rate = (self.fragment_stats['successful_reassemblies'] / total_fragment_groups) * 100
            print(f"   åˆ†ç‰‡é‡çµ„æˆåŠŸç‡: {fragment_success_rate:.1f}%")
        else:
            fragment_success_rate = 0
            print(f"   åˆ†ç‰‡é‡çµ„æˆåŠŸç‡: 0.0%")

        # åŒ¹é…å“è³ªåˆ†æ
        if self.unique_matches:
            scores = [m['score'] for m in self.unique_matches]
            avg_score = sum(scores) / len(scores)
            time_diffs = [m['time_diff'] for m in self.unique_matches]
            avg_time_diff = sum(time_diffs) / len(time_diffs)

            print(f"\nğŸ“ˆ åŒ¹é…å“è³ªåˆ†æ:")
            print(f"   å¹³å‡åŒ¹é…åˆ†æ•¸: {avg_score:.3f}")
            print(f"   å¹³å‡æ™‚é–“å·®: {avg_time_diff:.3f} ç§’")

            # åŒ¹é…é¡å‹çµ±è¨ˆ
            match_types = {}
            reassembled_matches = 0
            for match in self.unique_matches:
                match_type = match['match_type']
                match_types[match_type] = match_types.get(match_type, 0) + 1
                if match.get('is_reassembled', False):
                    reassembled_matches += 1

            print(f"\nğŸ·ï¸  åŒ¹é…é¡å‹åˆ†å¸ƒ:")
            for match_type, count in match_types.items():
                percentage = (count / len(self.unique_matches)) * 100
                print(f"   {match_type}: {count} å€‹ ({percentage:.1f}%)")

            print(
                f"   é‡çµ„å°åŒ…åŒ¹é…: {reassembled_matches} å€‹ ({(reassembled_matches / len(self.unique_matches)) * 100:.1f}%)")

        # å•é¡Œè¨ºæ–·
        print(f"\nğŸ” å•é¡Œè¨ºæ–·:")

        issues = []
        recommendations = []

        if restoration_rate < 20:
            issues.append("åˆ†ç‰‡é‡çµ„æ•ˆç‡æ¥µä½")
            recommendations.append("æª¢æŸ¥åˆ†ç‰‡è³‡æ–™å®Œæ•´æ€§å’ŒJSONæ ¼å¼")
            recommendations.append("å¢åŠ åˆ†ç‰‡è¶…æ™‚æ™‚é–“")

        if fragment_success_rate < 50:
            issues.append("åˆ†ç‰‡é‡çµ„å¤±æ•—ç‡éé«˜")
            recommendations.append("æª¢æŸ¥ç¶²è·¯å‚³è¼¸ç©©å®šæ€§")
            recommendations.append("å¢å¼·åˆ†ç‰‡éŒ¯èª¤è™•ç†æ©Ÿåˆ¶")

        if self.fragment_stats['timeout_groups'] > self.fragment_stats['complete_groups']:
            issues.append("å¤§é‡åˆ†ç‰‡çµ„è¶…æ™‚")
            recommendations.append("èª¿æ•´åˆ†ç‰‡è¶…æ™‚åƒæ•¸")
            recommendations.append("æª¢æŸ¥åˆ†ç‰‡ç™¼é€é–“éš”")

        if match_rate < 15:
            issues.append("æ•´é«”åŒ¹é…ç‡éä½")
            recommendations.append("æª¢æŸ¥æ™‚é–“åŒæ­¥å•é¡Œ")
            recommendations.append("èª¿æ•´åŒ¹é…ç®—æ³•åƒæ•¸")

        if len(issues) == 0:
            print("   âœ… æœªç™¼ç¾æ˜é¡¯å•é¡Œ")
        else:
            print(f"   ç™¼ç¾ {len(issues)} å€‹å•é¡Œ:")
            for i, issue in enumerate(issues, 1):
                print(f"   {i}. {issue}")

        print(f"\nğŸ’¡ æ”¹å–„å»ºè­°:")
        if len(recommendations) == 0:
            print("   ç³»çµ±é‹ä½œè‰¯å¥½ï¼Œç„¡éœ€ç‰¹åˆ¥æ”¹å–„")
        else:
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")

        # é¡å¤–çš„åˆ†ç‰‡å°ˆç”¨å»ºè­°
        print(f"\nğŸ§© åˆ†ç‰‡å°ˆç”¨å»ºè­°:")
        print(f"   1. è€ƒæ…®æ¸›å°‘åˆ†ç‰‡å¤§å°ä»¥æé«˜æˆåŠŸç‡")
        print(f"   2. å¢åŠ åˆ†ç‰‡é‡å‚³æ©Ÿåˆ¶")
        print(f"   3. å¯¦æ–½åˆ†ç‰‡é †åºæª¢æŸ¥")
        print(f"   4. æ·»åŠ åˆ†ç‰‡å®Œæ•´æ€§æ ¡é©—")
        print(f"   5. å„ªåŒ–åˆ†ç‰‡ç™¼é€æ™‚åº")

        # æ€§èƒ½è©•ä¼°
        self._evaluate_fragment_performance(match_rate, fragment_success_rate, restoration_rate)

        return match_rate >= 15  # é™ä½æˆåŠŸæ¨™æº–ï¼Œè€ƒæ…®åˆ°åˆ†ç‰‡è™•ç†çš„è¤‡é›œæ€§

    def _evaluate_fragment_performance(self, match_rate, fragment_success_rate, restoration_rate):
        """è©•ä¼°åˆ†ç‰‡è™•ç†æ€§èƒ½"""
        print(f"\nğŸ¯ åˆ†ç‰‡è™•ç†æ€§èƒ½è©•ä¼°:")

        # ç¶œåˆè©•åˆ†
        composite_score = (match_rate * 0.4 + fragment_success_rate * 0.4 + restoration_rate * 0.2)

        if composite_score >= 70:
            status = "ğŸ‰ å„ªç§€"
            conclusion = "åˆ†ç‰‡è™•ç†ç³»çµ±é‹ä½œè‰¯å¥½"
        elif composite_score >= 50:
            status = "âœ… è‰¯å¥½"
            conclusion = "åˆ†ç‰‡è™•ç†åŸºæœ¬æ­£å¸¸ï¼Œæœ‰æ”¹å–„ç©ºé–“"
        elif composite_score >= 30:
            status = "âš ï¸ éœ€è¦æ”¹å–„"
            conclusion = "åˆ†ç‰‡è™•ç†å­˜åœ¨å•é¡Œï¼Œéœ€è¦å„ªåŒ–"
        else:
            status = "âŒ éœ€è¦æª¢æŸ¥"
            conclusion = "åˆ†ç‰‡è™•ç†åš´é‡å•é¡Œï¼Œéœ€è¦å…¨é¢æª¢æŸ¥"

        print(f"   ç‹€æ…‹: {status}")
        print(f"   ç¶œåˆè©•åˆ†: {composite_score:.1f}åˆ†")
        print(f"   çµè«–: {conclusion}")

        # å…·é«”å»ºè­°
        if fragment_success_rate < 30:
            print(f"   ğŸš¨ ç·Šæ€¥: åˆ†ç‰‡é‡çµ„åš´é‡å¤±æ•—ï¼Œå»ºè­°æª¢æŸ¥:")
            print(f"      - JSONæ ¼å¼æ­£ç¢ºæ€§")
            print(f"      - Base64ç·¨ç¢¼å®Œæ•´æ€§")
            print(f"      - ç¶²è·¯å°åŒ…éºå¤±æƒ…æ³")

        if restoration_rate < 15:
            print(f"   ğŸš¨ ç·Šæ€¥: å°åŒ…é‚„åŸç‡æ¥µä½ï¼Œå»ºè­°:")
            print(f"      - æª¢æŸ¥åµŒå…¥å°åŒ…æ ¼å¼")
            print(f"      - é©—è­‰åˆ†ç‰‡è³‡æ–™å®Œæ•´æ€§")
            print(f"      - å¢å¼·éŒ¯èª¤è™•ç†æ©Ÿåˆ¶")


def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸ§© åˆ†ç‰‡å„ªåŒ–å°åŒ…æ¯”è¼ƒå·¥å…· v5.0")
    print("=" * 60)
    print("å°ˆé–€è§£æ±ºåˆ†ç‰‡è™•ç†å’Œé‡çµ„å•é¡Œ")

    comparator = FragmentOptimizedComparator()

    # é¡¯ç¤ºé…ç½®
    comparator.print_network_config()

    # è¨­å®šåƒæ•¸
    duration = int(input(f"\næ•ç²æ™‚é•· (ç§’ï¼Œå»ºè­°240): ") or "240")
    time_window = float(input(f"æ™‚é–“çª—å£ (ç§’ï¼Œå»ºè­°15): ") or "15")
    fragment_timeout = float(input(f"åˆ†ç‰‡è¶…æ™‚ (ç§’ï¼Œå»ºè­°30): ") or "30")

    comparator.time_window = time_window
    comparator.fragment_timeout = fragment_timeout

    print(f"\nâš ï¸  åˆ†ç‰‡å„ªåŒ–æ¨¡å¼æº–å‚™:")
    print(f"   å¢å¼·åˆ†ç‰‡è¿½è¹¤å’Œé‡çµ„")
    print(f"   æ™ºèƒ½è¶…æ™‚æ¸…ç†æ©Ÿåˆ¶")
    print(f"   è©³ç´°åˆ†ç‰‡ç‹€æ…‹è¨ºæ–·")
    print(f"   æå‡åˆ†ç‰‡æˆåŠŸç‡")

    input("\næŒ‰Enteré–‹å§‹åˆ†ç‰‡å„ªåŒ–æ•ç²...")

    try:
        success = comparator.start_synchronized_capture(duration)

        print(f"\nâœ… åˆ†ç‰‡å„ªåŒ–æ¯”è¼ƒå®Œæˆ!")

        if success:
            print(f"ğŸ‰ åˆ†ç‰‡è™•ç†æ€§èƒ½å¯æ¥å—ï¼")
        else:
            print(f"âš ï¸  åˆ†ç‰‡è™•ç†éœ€è¦é€²ä¸€æ­¥å„ªåŒ–")
            print(f"å»ºè­°æª¢æŸ¥ç¶²è·¯ç’°å¢ƒå’Œç³»çµ±é…ç½®")

    except KeyboardInterrupt:
        print(f"\nâš ï¸  æ¯”è¼ƒè¢«ä¸­æ–·")
    except Exception as e:
        print(f"âŒ åˆ†ç‰‡å„ªåŒ–æ¯”è¼ƒç™¼ç”ŸéŒ¯èª¤: {e}")


if __name__ == "__main__":
    main()