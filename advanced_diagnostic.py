#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import base64
import hashlib
from scapy.all import *
from collections import defaultdict
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime

class AdvancedPacketDiagnostic:
    """æ·±å…¥è¨ºæ–·å°åŒ…æµå’Œæ™‚é–“åŒæ­¥å•é¡Œ"""
    
    def __init__(self):
        self.issues_found = []
        self.recommendations = []
    
    def analyze_packet_flow_timing(self, incoming_pcap, embedded_pcap, restored_pcap):
        """åˆ†æå°åŒ…æµçš„æ™‚é–“åŒæ­¥å•é¡Œ"""
        print("ğŸ” æ·±å…¥è¨ºæ–·å°åŒ…æµæ™‚é–“åŒæ­¥å•é¡Œ...")
        print("=" * 70)
        
        # è®€å–PCAPæª”æ¡ˆ
        try:
            incoming_packets = rdpcap(incoming_pcap)
            embedded_packets = rdpcap(embedded_pcap)
            restored_packets = rdpcap(restored_pcap)
            
            print(f"ğŸ“‚ æª”æ¡ˆè¼‰å…¥æˆåŠŸ:")
            print(f"   ä¾†æºå°åŒ…: {len(incoming_packets)} å€‹")
            print(f"   åµŒå…¥å°åŒ…: {len(embedded_packets)} å€‹")
            print(f"   é‚„åŸå°åŒ…: {len(restored_packets)} å€‹")
            
        except Exception as e:
            print(f"âŒ æª”æ¡ˆè¼‰å…¥å¤±æ•—: {e}")
            return
        
        # 1. æ™‚é–“åˆ†æ
        self._analyze_packet_timing(incoming_packets, embedded_packets)
        
        # 2. å°åŒ…å¤§å°åˆ†å¸ƒåˆ†æ
        self._analyze_packet_size_distribution(incoming_packets, embedded_packets, restored_packets)
        
        # 3. å°åŒ…å…§å®¹åˆ†æ
        self._analyze_packet_content_patterns(incoming_packets, embedded_packets)
        
        # 4. åµŒå…¥å°åŒ…è§£æåˆ†æ
        self._analyze_embedded_packet_parsing(embedded_packets)
        
        # 5. ç”Ÿæˆæ”¹å–„å»ºè­°
        self._generate_improvement_recommendations()
        
    def _analyze_packet_timing(self, incoming_packets, embedded_packets):
        """åˆ†æå°åŒ…æ™‚é–“åˆ†å¸ƒ"""
        print(f"\nâ° æ™‚é–“åˆ†æ:")
        
        # ç²å–æ™‚é–“æˆ³
        incoming_times = []
        embedded_times = []
        
        for packet in incoming_packets:
            if hasattr(packet, 'time'):
                incoming_times.append(float(packet.time))
        
        for packet in embedded_packets:
            if hasattr(packet, 'time'):
                embedded_times.append(float(packet.time))
        
        if incoming_times and embedded_times:
            incoming_start = min(incoming_times)
            incoming_end = max(incoming_times)
            embedded_start = min(embedded_times)
            embedded_end = max(embedded_times)
            
            print(f"   ä¾†æºå°åŒ…æ™‚é–“ç¯„åœ: {datetime.fromtimestamp(incoming_start)} - {datetime.fromtimestamp(incoming_end)}")
            print(f"   åµŒå…¥å°åŒ…æ™‚é–“ç¯„åœ: {datetime.fromtimestamp(embedded_start)} - {datetime.fromtimestamp(embedded_end)}")
            
            # è¨ˆç®—æ™‚é–“é‡ç–Š
            overlap_start = max(incoming_start, embedded_start)
            overlap_end = min(incoming_end, embedded_end)
            
            if overlap_start < overlap_end:
                overlap_duration = overlap_end - overlap_start
                total_duration = max(incoming_end, embedded_end) - min(incoming_start, embedded_start)
                overlap_percentage = (overlap_duration / total_duration) * 100
                
                print(f"   æ™‚é–“é‡ç–Š: {overlap_percentage:.1f}% ({overlap_duration:.1f}ç§’)")
                
                if overlap_percentage < 80:
                    self.issues_found.append(f"æ™‚é–“é‡ç–Šä¸è¶³: {overlap_percentage:.1f}%")
                    self.recommendations.append("å¢åŠ æ•ç²æ™‚é–“æˆ–ç¢ºä¿åŒæ™‚é–‹å§‹æ•ç²")
            else:
                print(f"   âŒ æ²’æœ‰æ™‚é–“é‡ç–Š!")
                self.issues_found.append("ä¾†æºå’ŒåµŒå…¥å°åŒ…æ²’æœ‰æ™‚é–“é‡ç–Š")
                self.recommendations.append("ç¢ºä¿åŒæ™‚æ•ç²ä¾†æºå’ŒåµŒå…¥å°åŒ…")
        
    def _analyze_packet_size_distribution(self, incoming_packets, embedded_packets, restored_packets):
        """åˆ†æå°åŒ…å¤§å°åˆ†å¸ƒ"""
        print(f"\nğŸ“ å°åŒ…å¤§å°åˆ†å¸ƒåˆ†æ:")
        
        # æ”¶é›†å¤§å°è³‡æ–™
        incoming_sizes = [len(bytes(p)) for p in incoming_packets]
        embedded_sizes = [len(bytes(p)) for p in embedded_packets]  
        restored_sizes = [len(bytes(p)) for p in restored_packets]
        
        # çµ±è¨ˆåˆ†æ
        print(f"   ä¾†æºå°åŒ…å¤§å°:")
        self._print_size_stats(incoming_sizes)
        
        print(f"   åµŒå…¥å°åŒ…å¤§å°:")
        self._print_size_stats(embedded_sizes)
        
        print(f"   é‚„åŸå°åŒ…å¤§å°:")
        self._print_size_stats(restored_sizes)
        
        # æª¢æŸ¥å¤§å°åˆ†å¸ƒå•é¡Œ
        incoming_avg = sum(incoming_sizes) / len(incoming_sizes) if incoming_sizes else 0
        restored_avg = sum(restored_sizes) / len(restored_sizes) if restored_sizes else 0
        
        if abs(incoming_avg - restored_avg) > 100:
            self.issues_found.append(f"å°åŒ…å¤§å°å·®ç•°éå¤§: ä¾†æºå¹³å‡{incoming_avg:.0f} vs é‚„åŸå¹³å‡{restored_avg:.0f}")
            self.recommendations.append("æª¢æŸ¥æ˜¯å¦æ•ç²äº†ä¸åŒé¡å‹çš„å°åŒ…")
    
    def _print_size_stats(self, sizes):
        """æ‰“å°å¤§å°çµ±è¨ˆ"""
        if not sizes:
            print("      ç„¡æ•¸æ“š")
            return
            
        sizes.sort()
        avg = sum(sizes) / len(sizes)
        median = sizes[len(sizes)//2]
        min_size = min(sizes)
        max_size = max(sizes)
        
        print(f"      å¹³å‡: {avg:.0f} bytes, ä¸­ä½æ•¸: {median} bytes")
        print(f"      ç¯„åœ: {min_size} - {max_size} bytes")
        
        # å¤§å°åˆ†å¸ƒ
        size_ranges = {
            "å°å°åŒ…(<100)": len([s for s in sizes if s < 100]),
            "ä¸­å°åŒ…(100-1000)": len([s for s in sizes if 100 <= s < 1000]),
            "å¤§å°åŒ…(1000+)": len([s for s in sizes if s >= 1000])
        }
        
        for range_name, count in size_ranges.items():
            percentage = (count / len(sizes)) * 100
            print(f"      {range_name}: {count} å€‹ ({percentage:.1f}%)")
    
    def _analyze_packet_content_patterns(self, incoming_packets, embedded_packets):
        """åˆ†æå°åŒ…å…§å®¹æ¨¡å¼"""
        print(f"\nğŸ” å°åŒ…å…§å®¹æ¨¡å¼åˆ†æ:")
        
        # åˆ†æä¾†æºå°åŒ…æ¨¡å¼
        print(f"   ä¾†æºå°åŒ…æ¨¡å¼:")
        incoming_patterns = self._extract_packet_patterns(incoming_packets)
        for pattern, count in incoming_patterns.items():
            print(f"      {pattern}: {count} å€‹")
        
        # åˆ†æåµŒå…¥å°åŒ…ä¸­çš„åŸå§‹å°åŒ…æ¨¡å¼
        print(f"   åµŒå…¥å°åŒ…ä¸­çš„åŸå§‹å°åŒ…æ¨¡å¼:")
        embedded_original_patterns = self._extract_embedded_original_patterns(embedded_packets)
        for pattern, count in embedded_original_patterns.items():
            print(f"      {pattern}: {count} å€‹")
        
        # æª¢æŸ¥æ¨¡å¼åŒ¹é…
        common_patterns = set(incoming_patterns.keys()) & set(embedded_original_patterns.keys())
        if common_patterns:
            print(f"   âœ… å…±åŒæ¨¡å¼: {len(common_patterns)} å€‹")
            for pattern in common_patterns:
                print(f"      {pattern}: ä¾†æº{incoming_patterns[pattern]} vs åµŒå…¥{embedded_original_patterns[pattern]}")
        else:
            print(f"   âŒ æ²’æœ‰å…±åŒçš„å°åŒ…æ¨¡å¼")
            self.issues_found.append("ä¾†æºå’ŒåµŒå…¥å°åŒ…æ²’æœ‰å…±åŒæ¨¡å¼")
            self.recommendations.append("æª¢æŸ¥æ˜¯å¦æ•ç²äº†ç›¸åŒä¾†æºçš„å°åŒ…")
    
    def _extract_packet_patterns(self, packets):
        """æå–å°åŒ…æ¨¡å¼"""
        patterns = defaultdict(int)
        
        for packet in packets:
            pattern_key = ""
            
            if IP in packet:
                pattern_key += f"IP({packet[IP].src}->{packet[IP].dst})"
                
            if TCP in packet:
                pattern_key += f"_TCP({packet[TCP].sport}->{packet[TCP].dport})"
                
            if Raw in packet:
                payload_size = len(packet[Raw].load)
                pattern_key += f"_Payload({payload_size})"
            else:
                pattern_key += "_NoPayload"
                
            patterns[pattern_key] += 1
            
        return dict(patterns)
    
    def _extract_embedded_original_patterns(self, embedded_packets):
        """å¾åµŒå…¥å°åŒ…ä¸­æå–åŸå§‹å°åŒ…æ¨¡å¼"""
        patterns = defaultdict(int)
        
        for packet in embedded_packets:
            if Raw not in packet:
                continue
                
            try:
                payload = packet[Raw].load.decode('utf-8')
                data = json.loads(payload)
                
                if "fragment_info" in data:
                    # è·³éåˆ†ç‰‡ï¼Œåªè™•ç†å®Œæ•´å°åŒ…
                    continue
                    
                if "original_packet" in data:
                    orig_data = data["original_packet"]
                    
                    pattern_key = ""
                    pattern_key += f"IP({orig_data.get('original_src', 'unknown')}->{orig_data.get('original_dst', 'unknown')})"
                    
                    if 'tcp_sport' in orig_data and 'tcp_dport' in orig_data:
                        pattern_key += f"_TCP({orig_data['tcp_sport']}->{orig_data['tcp_dport']})"
                    
                    size = orig_data.get('length', 0)
                    pattern_key += f"_Size({size})"
                    
                    patterns[pattern_key] += 1
                    
            except:
                continue
                
        return dict(patterns)
    
    def _analyze_embedded_packet_parsing(self, embedded_packets):
        """åˆ†æåµŒå…¥å°åŒ…è§£æç‹€æ³"""
        print(f"\nğŸ“¦ åµŒå…¥å°åŒ…è§£æåˆ†æ:")
        
        total_embedded = len(embedded_packets)
        parseable_count = 0
        fragment_count = 0
        complete_count = 0
        error_count = 0
        
        for packet in embedded_packets:
            if Raw not in packet:
                continue
                
            try:
                payload = packet[Raw].load.decode('utf-8')
                data = json.loads(payload)
                parseable_count += 1
                
                if "fragment_info" in data:
                    fragment_count += 1
                elif "original_packet" in data:
                    complete_count += 1
                    
            except:
                error_count += 1
        
        print(f"   ç¸½åµŒå…¥å°åŒ…: {total_embedded}")
        print(f"   å¯è§£æ: {parseable_count} ({(parseable_count/total_embedded)*100:.1f}%)")
        print(f"   åˆ†ç‰‡å°åŒ…: {fragment_count} ({(fragment_count/total_embedded)*100:.1f}%)")
        print(f"   å®Œæ•´å°åŒ…: {complete_count} ({(complete_count/total_embedded)*100:.1f}%)")
        print(f"   è§£æéŒ¯èª¤: {error_count} ({(error_count/total_embedded)*100:.1f}%)")
        
        if error_count > 0:
            self.issues_found.append(f"æœ‰{error_count}å€‹åµŒå…¥å°åŒ…ç„¡æ³•è§£æ")
            self.recommendations.append("æª¢æŸ¥åµŒå…¥å°åŒ…çš„JSONæ ¼å¼")
        
        # åˆ†æåˆ†ç‰‡çµ„åˆç‹€æ³
        if fragment_count > 0:
            self._analyze_fragment_completion(embedded_packets)
    
    def _analyze_fragment_completion(self, embedded_packets):
        """åˆ†æåˆ†ç‰‡å®Œæˆç‹€æ³"""
        print(f"\nğŸ§© åˆ†ç‰‡å®Œæˆç‹€æ³åˆ†æ:")
        
        fragments = defaultdict(dict)
        
        for packet in embedded_packets:
            if Raw not in packet:
                continue
                
            try:
                payload = packet[Raw].load.decode('utf-8')
                data = json.loads(payload)
                
                if "fragment_info" in data:
                    frag_info = data["fragment_info"]
                    frag_uuid = frag_info["fragment_uuid"]
                    frag_index = frag_info["fragment_index"]
                    total_frags = frag_info["total_fragments"]
                    
                    if frag_uuid not in fragments:
                        fragments[frag_uuid] = {
                            'total': total_frags,
                            'received': set(),
                            'expected': set(range(total_frags))
                        }
                    
                    fragments[frag_uuid]['received'].add(frag_index)
                    
            except:
                continue
        
        complete_fragments = 0
        incomplete_fragments = 0
        
        for frag_uuid, frag_data in fragments.items():
            if frag_data['received'] == frag_data['expected']:
                complete_fragments += 1
            else:
                incomplete_fragments += 1
                missing = frag_data['expected'] - frag_data['received']
                print(f"   âŒ åˆ†ç‰‡ {frag_uuid[:8]}... ç¼ºå°‘: {sorted(missing)}")
        
        print(f"   å®Œæ•´åˆ†ç‰‡çµ„: {complete_fragments}")
        print(f"   ä¸å®Œæ•´åˆ†ç‰‡çµ„: {incomplete_fragments}")
        
        if incomplete_fragments > 0:
            self.issues_found.append(f"æœ‰{incomplete_fragments}å€‹åˆ†ç‰‡çµ„ä¸å®Œæ•´")
            self.recommendations.append("æª¢æŸ¥ç¶²è·¯å‚³è¼¸æ˜¯å¦æœ‰å°åŒ…éºå¤±")
    
    def _generate_improvement_recommendations(self):
        """ç”Ÿæˆæ”¹å–„å»ºè­°"""
        print(f"\n" + "=" * 70)
        print("ğŸ¯ è¨ºæ–·çµæœå’Œæ”¹å–„å»ºè­°")
        print("=" * 70)
        
        if not self.issues_found:
            print("âœ… æ²’æœ‰ç™¼ç¾æ˜é¡¯å•é¡Œ")
        else:
            print(f"âŒ ç™¼ç¾ {len(self.issues_found)} å€‹å•é¡Œ:")
            for i, issue in enumerate(self.issues_found, 1):
                print(f"   {i}. {issue}")
        
        print(f"\nğŸ”§ æ”¹å–„å»ºè­°:")
        if not self.recommendations:
            print("   ç³»çµ±é‹ä½œæ­£å¸¸ï¼Œç„¡éœ€æ”¹å–„")
        else:
            for i, rec in enumerate(self.recommendations, 1):
                print(f"   {i}. {rec}")
        
        # é¡å¤–å»ºè­°
        print(f"\nğŸ’¡ é€²éšå»ºè­°:")
        print("   1. ä½¿ç”¨æ›´é•·çš„æ•ç²æ™‚é–“ç¢ºä¿æ•¸æ“šå……è¶³")
        print("   2. åœ¨æ•ç²å‰ç¢ºä¿æµé‡ç©©å®š")
        print("   3. è€ƒæ…®ä½¿ç”¨å°åŒ…å…§å®¹è€Œéé›œæ¹Šé€²è¡Œæ¯”è¼ƒ")
        print("   4. æª¢æŸ¥ç¶²è·¯å»¶é²å’Œå°åŒ…é †åº")
        print("   5. è€ƒæ…®å¯¦ä½œå³æ™‚åŒæ­¥æ•ç²æ©Ÿåˆ¶")

def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸ” æ·±å…¥å°åŒ…æµè¨ºæ–·å·¥å…·")
    print("=" * 50)
    
    # ä½¿ç”¨æœ€æ–°çš„PCAPæª”æ¡ˆ
    incoming_pcap = "incoming_packets_5006_20250526_223159.pcap"
    embedded_pcap = "embedded_packets_20_20250526_223159.pcap"
    restored_pcap = "restored_packets_20250526_223159.pcap"
    
    diagnostic = AdvancedPacketDiagnostic()
    
    try:
        diagnostic.analyze_packet_flow_timing(incoming_pcap, embedded_pcap, restored_pcap)
    except FileNotFoundError as e:
        print(f"âŒ æª”æ¡ˆæœªæ‰¾åˆ°: {e}")
        print("è«‹ç¢ºèªPCAPæª”æ¡ˆè·¯å¾‘æ­£ç¢º")
    except Exception as e:
        print(f"âŒ è¨ºæ–·éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()