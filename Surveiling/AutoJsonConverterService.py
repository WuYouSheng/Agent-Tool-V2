#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import base64
import hashlib
import time
import os
import threading
import signal
from datetime import datetime
from scapy.all import *
from collections import defaultdict
import shutil
import glob

class AutoJsonConverterService:
    """è‡ªå‹•åŒ–PCAPåˆ°JSONè½‰æ›æœå‹™"""
    
    def __init__(self, config_path="./config_surveiling_sample.json"):
        self.config_path = config_path
        self.config = {}
        
        # æœå‹™è¨­å®š
        self.time_gen_gap = 5  # é è¨­5ç§’
        self.pcap_input_dir = "PCAP"
        self.json_output_dir = "JSON"
        self.processed_files = set()  # è¿½è¹¤å·²è™•ç†çš„æª”æ¡ˆ
        self.is_running = False
        
        # çµ±è¨ˆè³‡è¨Š
        self.conversion_stats = {
            'total_files_processed': 0,
            'total_packets_converted': 0,
            'total_restored_packets': 0,
            'failed_conversions': 0,
            'service_start_time': None,
            'last_conversion_time': None
        }
        
        # åˆ†ç‰‡è™•ç†
        self.fragment_buffer = defaultdict(dict)
        
        # è¨­å®šä¿¡è™Ÿè™•ç†
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def load_config(self):
        """è¼‰å…¥é…ç½®æª”æ¡ˆ"""
        try:
            with open(self.config_path,encoding='utf-8') as f:
                self.config = json.load(f)
            
            # è®€å–time_gen_gapè¨­å®š
            self.time_gen_gap = self.config.get("time_gen_gap", 5)
            
            # å¯é¸çš„è‡ªè¨‚ç›®éŒ„è¨­å®š
            self.pcap_input_dir = self.config.get("pcap_input_directory", "PCAP")
            self.json_output_dir = self.config.get("json_output_directory", "JSON")
            
            print("=== JSONè½‰æ›æœå‹™é…ç½®è¼‰å…¥æˆåŠŸ ===")
            print(f"é…ç½®æª”æ¡ˆ: {self.config_path}")
            print(f"è½‰æ›é–“éš”: {self.time_gen_gap} ç§’")
            print(f"PCAPè¼¸å…¥ç›®éŒ„: {self.pcap_input_dir}")
            print(f"JSONè¼¸å‡ºç›®éŒ„: {self.json_output_dir}")
            print("=" * 40)
            
            return True
            
        except FileNotFoundError:
            print(f"âŒ é…ç½®æª”æ¡ˆæœªæ‰¾åˆ°: {self.config_path}")
            print("ä½¿ç”¨é è¨­è¨­å®šç¹¼çºŒé‹è¡Œ...")
            return True  # ä½¿ç”¨é è¨­è¨­å®šç¹¼çºŒ
        except json.JSONDecodeError as e:
            print(f"âŒ é…ç½®æª”æ¡ˆJSONæ ¼å¼éŒ¯èª¤: {e}")
            return False
        except Exception as e:
            print(f"âŒ è¼‰å…¥é…ç½®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False
    
    def _ensure_directories(self):
        """ç¢ºä¿å¿…è¦ç›®éŒ„å­˜åœ¨"""
        try:
            # ç¢ºä¿JSONè¼¸å‡ºç›®éŒ„å­˜åœ¨
            if not os.path.exists(self.json_output_dir):
                os.makedirs(self.json_output_dir)
                print(f"âœ… å‰µå»ºJSONè¼¸å‡ºç›®éŒ„: {self.json_output_dir}")
            
            # ç¢ºä¿PCAPè¼¸å…¥ç›®éŒ„å­˜åœ¨
            if not os.path.exists(self.pcap_input_dir):
                os.makedirs(self.pcap_input_dir)
                print(f"âœ… å‰µå»ºPCAPè¼¸å…¥ç›®éŒ„: {self.pcap_input_dir}")
                print(f"âš ï¸  æ³¨æ„: PCAPç›®éŒ„æ˜¯ç©ºçš„ï¼Œç­‰å¾…æª”æ¡ˆ...")
            
            return True
            
        except Exception as e:
            print(f"âŒ å‰µå»ºç›®éŒ„å¤±æ•—: {e}")
            return False
    
    def _signal_handler(self, signum, frame):
        """è™•ç†ç³»çµ±ä¿¡è™Ÿ"""
        print(f"\næ”¶åˆ°çµ‚æ­¢ä¿¡è™Ÿ {signum}ï¼Œæ­£åœ¨å®‰å…¨é—œé–‰...")
        self.shutdown()
    
    def start_service(self):
        """å•Ÿå‹•è‡ªå‹•è½‰æ›æœå‹™"""
        try:
            print("ğŸš€ å•Ÿå‹•è‡ªå‹•JSONè½‰æ›æœå‹™...")
            
            # è¼‰å…¥é…ç½®
            if not self.load_config():
                return False
            
            # ç¢ºä¿ç›®éŒ„å­˜åœ¨
            if not self._ensure_directories():
                return False
            
            self.is_running = True
            self.conversion_stats['service_start_time'] = time.time()
            
            print("âœ… JSONè½‰æ›æœå‹™åˆå§‹åŒ–å®Œæˆ")
            print(f"ğŸ”„ é–‹å§‹ç›£æ§ {self.pcap_input_dir} ç›®éŒ„...")
            print(f"   è½‰æ›é–“éš”: {self.time_gen_gap} ç§’")
            print("   æŒ‰ Ctrl+C åœæ­¢æœå‹™\n")
            
            # å•Ÿå‹•ç›£æ§åŸ·è¡Œç·’
            monitor_thread = threading.Thread(target=self._monitor_and_convert, daemon=True)
            monitor_thread.start()
            
            # ä¸»åŸ·è¡Œç·’ç­‰å¾…
            try:
                while self.is_running:
                    time.sleep(1)
            except KeyboardInterrupt:
                pass
            
            return True
            
        except Exception as e:
            print(f"âŒ å•Ÿå‹•æœå‹™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False
    
    def _monitor_and_convert(self):
        """ç›£æ§ä¸¦è½‰æ›PCAPæª”æ¡ˆ"""
        print(f"ğŸ”„ ç›£æ§åŸ·è¡Œç·’å•Ÿå‹• (é–“éš”: {self.time_gen_gap}ç§’)")
        
        while self.is_running:
            try:
                # æƒæPCAPç›®éŒ„ä¸­çš„æ–°æª”æ¡ˆ
                pcap_pattern = os.path.join(self.pcap_input_dir, "*.pcap")
                
                # ä½¿ç”¨os.listdirå’Œfnmatchä½œç‚ºå‚™é¸æ–¹æ¡ˆ
                try:
                    import fnmatch
                    all_files = os.listdir(self.pcap_input_dir)
                    pcap_files = [os.path.join(self.pcap_input_dir, f) 
                                 for f in all_files if fnmatch.fnmatch(f, "*.pcap")]
                except ImportError:
                    # å¦‚æœfnmatchä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºæœ¬å­—ä¸²æª¢æŸ¥
                    all_files = os.listdir(self.pcap_input_dir)
                    pcap_files = [os.path.join(self.pcap_input_dir, f) 
                                 for f in all_files if f.endswith(".pcap")]
                
                # æ‰¾å‡ºæ–°æª”æ¡ˆ
                new_files = []
                for pcap_file in pcap_files:
                    if pcap_file not in self.processed_files:
                        new_files.append(pcap_file)
                
                # è™•ç†æ–°æª”æ¡ˆ
                if new_files:
                    print(f"\nğŸ” ç™¼ç¾ {len(new_files)} å€‹æ–°PCAPæª”æ¡ˆ")
                    for pcap_file in new_files:
                        self._convert_single_file(pcap_file)
                        self.processed_files.add(pcap_file)
                
                # ç­‰å¾…ä¸‹ä¸€æ¬¡æª¢æŸ¥
                time.sleep(self.time_gen_gap)
                
            except Exception as e:
                print(f"âŒ ç›£æ§åŸ·è¡Œç·’éŒ¯èª¤: {e}")
                import traceback
                traceback.print_exc()
                time.sleep(5)  # éŒ¯èª¤æ™‚ç­‰å¾…5ç§’
    
    def _convert_single_file(self, pcap_file):
        """è½‰æ›å–®å€‹PCAPæª”æ¡ˆ"""
        try:
            print(f"ğŸ”„ è™•ç†æª”æ¡ˆ: {os.path.basename(pcap_file)}")
            
            # è®€å–PCAPæª”æ¡ˆ
            packets = rdpcap(pcap_file)
            
            if len(packets) == 0:
                print(f"   âš ï¸  æª”æ¡ˆç‚ºç©ºï¼Œè·³é")
                return
            
            # ç¢ºå®šæª”æ¡ˆé¡å‹å’Œè½‰æ›æ¨¡å¼
            file_basename = os.path.basename(pcap_file)
            if file_basename.startswith('restored_'):
                conversion_mode = 'restored_analysis'
                print(f"   ğŸ“‹ æª”æ¡ˆé¡å‹: é‚„åŸå°åŒ…")
            elif file_basename.startswith('received_'):
                conversion_mode = 'embedded_analysis'
                print(f"   ğŸ“‹ æª”æ¡ˆé¡å‹: æ¥æ”¶å°åŒ… (åµŒå…¥)")
            else:
                conversion_mode = 'general_analysis'
                print(f"   ğŸ“‹ æª”æ¡ˆé¡å‹: ä¸€èˆ¬å°åŒ…")
            
            # è½‰æ›å°åŒ…
            converted_data = self._convert_packets_to_json(packets, pcap_file, conversion_mode)
            
            # ç”Ÿæˆè¼¸å‡ºæª”æ¡ˆå
            output_filename = self._generate_json_filename(pcap_file, conversion_mode)
            output_path = os.path.join(self.json_output_dir, output_filename)
            
            # å¯«å…¥JSONæª”æ¡ˆ
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(converted_data, f, indent=2, ensure_ascii=False, default=str)
            
            # æ›´æ–°çµ±è¨ˆ
            self.conversion_stats['total_files_processed'] += 1
            self.conversion_stats['total_packets_converted'] += len(packets)
            self.conversion_stats['last_conversion_time'] = time.time()
            
            file_size = os.path.getsize(output_path)
            print(f"   âœ… è½‰æ›å®Œæˆ: {output_filename}")
            print(f"   ğŸ“Š {len(packets)} å€‹å°åŒ… â†’ {file_size/1024:.1f} KB JSON")
            
        except Exception as e:
            print(f"   âŒ è½‰æ›å¤±æ•—: {e}")
            self.conversion_stats['failed_conversions'] += 1
    
    def _convert_packets_to_json(self, packets, source_file, mode):
        """å°‡å°åŒ…è½‰æ›ç‚ºJSONæ ¼å¼"""
        timestamp = datetime.now().isoformat()
        
        result = {
            "metadata": {
                "conversion_time": timestamp,
                "source_file": os.path.basename(source_file),
                "conversion_mode": mode,
                "total_packets": len(packets),
                "converter_version": "auto_service_v1.0"
            },
            "packets": [],
            "statistics": {},
            "embedded_analysis": {} if mode == 'embedded_analysis' else None,
            "restoration_analysis": {} if mode == 'restored_analysis' else None
        }
        
        if mode == 'embedded_analysis':
            return self._analyze_embedded_packets(packets, result)
        elif mode == 'restored_analysis':
            return self._analyze_restored_packets(packets, result)
        else:
            return self._analyze_general_packets(packets, result)
    
    def _analyze_embedded_packets(self, packets, result):
        """åˆ†æåµŒå…¥å°åŒ…"""
        fragments = {}
        embedded_packets = []
        parsing_errors = []
        
        for i, packet in enumerate(packets):
            try:
                if Raw in packet:
                    payload = packet[Raw].load.decode('utf-8')
                    embedded_data = json.loads(payload)
                    
                    packet_info = {
                        "packet_index": i,
                        "timestamp": float(packet.time) if hasattr(packet, 'time') else time.time(),
                        "source": f"{packet[IP].src}:{packet[TCP].sport}" if IP in packet and TCP in packet else "unknown",
                        "size": len(bytes(packet)),
                        "embedded_content": embedded_data
                    }
                    
                    # è™•ç†åˆ†ç‰‡
                    if "fragment_info" in embedded_data:
                        fragment_info = embedded_data["fragment_info"]
                        fragment_uuid = fragment_info["fragment_uuid"]
                        
                        if fragment_uuid not in fragments:
                            fragments[fragment_uuid] = {
                                "total_fragments": fragment_info["total_fragments"],
                                "received_fragments": [],
                                "is_complete": False,
                                "first_seen": packet_info["timestamp"]
                            }
                        
                        fragments[fragment_uuid]["received_fragments"].append({
                            "index": fragment_info["fragment_index"],
                            "packet_index": i,
                            "size": len(embedded_data.get("data", "")),
                            "timestamp": packet_info["timestamp"]
                        })
                        
                        # æª¢æŸ¥å®Œæ•´æ€§
                        if len(fragments[fragment_uuid]["received_fragments"]) == fragment_info["total_fragments"]:
                            fragments[fragment_uuid]["is_complete"] = True
                            # å˜—è©¦é‡çµ„
                            restored = self._try_reassemble_fragment(fragment_uuid, fragments[fragment_uuid], embedded_data)
                            if restored:
                                fragments[fragment_uuid]["restoration_success"] = True
                                self.conversion_stats['total_restored_packets'] += 1
                    else:
                        embedded_packets.append(packet_info)
                        
            except json.JSONDecodeError as e:
                parsing_errors.append({
                    "packet_index": i,
                    "error": "JSON decode error",
                    "details": str(e)
                })
            except Exception as e:
                parsing_errors.append({
                    "packet_index": i,
                    "error": "General parsing error",
                    "details": str(e)
                })
        
        # æ›´æ–°çµæœ
        result["embedded_analysis"] = {
            "embedded_packets": embedded_packets,
            "fragments": fragments,
            "parsing_errors": parsing_errors,
            "statistics": {
                "total_embedded": len(embedded_packets),
                "total_fragments": len(fragments),
                "complete_fragments": sum(1 for f in fragments.values() if f["is_complete"]),
                "parsing_errors": len(parsing_errors)
            }
        }
        
        return result
    
    def _analyze_restored_packets(self, packets, result):
        """åˆ†æé‚„åŸå°åŒ…"""
        restored_packets = []
        
        for i, packet in enumerate(packets):
            try:
                packet_analysis = {
                    "packet_index": i,
                    "timestamp": float(packet.time) if hasattr(packet, 'time') else time.time(),
                    "size": len(bytes(packet)),
                    "hash": hashlib.sha256(bytes(packet)).hexdigest(),
                    "layers": self._get_packet_layers(packet),
                    "protocol_analysis": {}
                }
                
                # å”è­°åˆ†æ
                if IP in packet:
                    packet_analysis["protocol_analysis"]["ip"] = {
                        "src": packet[IP].src,
                        "dst": packet[IP].dst,
                        "protocol": packet[IP].proto,
                        "ttl": packet[IP].ttl
                    }
                
                if TCP in packet:
                    packet_analysis["protocol_analysis"]["tcp"] = {
                        "src_port": packet[TCP].sport,
                        "dst_port": packet[TCP].dport,
                        "flags": int(packet[TCP].flags),
                        "flags_readable": self._tcp_flags_to_string(packet[TCP].flags)
                    }
                
                if Raw in packet:
                    payload = packet[Raw].load
                    packet_analysis["payload"] = {
                        "size": len(payload),
                        "encoding": self._detect_encoding(payload),
                        "preview": payload[:100].hex()
                    }
                
                restored_packets.append(packet_analysis)
                
            except Exception as e:
                print(f"   âš ï¸  å°åŒ… {i} åˆ†æå¤±æ•—: {e}")
        
        result["restoration_analysis"] = {
            "restored_packets": restored_packets,
            "statistics": {
                "total_restored": len(restored_packets),
                "unique_sources": len(set(p["protocol_analysis"].get("ip", {}).get("src", "") for p in restored_packets)),
                "unique_destinations": len(set(p["protocol_analysis"].get("ip", {}).get("dst", "") for p in restored_packets))
            }
        }
        
        return result
    
    def _analyze_general_packets(self, packets, result):
        """åˆ†æä¸€èˆ¬å°åŒ…"""
        packet_list = []
        
        for i, packet in enumerate(packets):
            try:
                packet_dict = {
                    "index": i,
                    "timestamp": float(packet.time) if hasattr(packet, 'time') else time.time(),
                    "size": len(bytes(packet)),
                    "summary": packet.summary(),
                    "hash": hashlib.sha256(bytes(packet)).hexdigest(),
                    "layers": self._get_packet_layers(packet)
                }
                
                packet_list.append(packet_dict)
                
            except Exception as e:
                print(f"   âš ï¸  å°åŒ… {i} è™•ç†å¤±æ•—: {e}")
        
        result["packets"] = packet_list
        result["statistics"] = {
            "total_packets": len(packet_list),
            "average_size": sum(p["size"] for p in packet_list) / len(packet_list) if packet_list else 0
        }
        
        return result
    
    def _try_reassemble_fragment(self, fragment_uuid, fragment_info, sample_data):
        """å˜—è©¦é‡çµ„åˆ†ç‰‡ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        try:
            # é€™è£¡åªæ˜¯æ¨™è¨˜é‡çµ„å˜—è©¦ï¼Œå¯¦éš›é‡çµ„é‚è¼¯å¯ä»¥æ›´è¤‡é›œ
            return fragment_info["is_complete"]
        except:
            return False
    
    def _get_packet_layers(self, packet):
        """ç²å–å°åŒ…å±¤ç´š"""
        layers = []
        current = packet
        while current:
            layer_name = current.__class__.__name__
            layers.append(layer_name)
            if hasattr(current, 'payload') and current.payload:
                current = current.payload
            else:
                break
        return layers
    
    def _tcp_flags_to_string(self, flags):
        """TCP flagsè½‰å­—ä¸²"""
        flag_names = []
        if flags & 0x01: flag_names.append("FIN")
        if flags & 0x02: flag_names.append("SYN")
        if flags & 0x04: flag_names.append("RST")
        if flags & 0x08: flag_names.append("PSH")
        if flags & 0x10: flag_names.append("ACK")
        if flags & 0x20: flag_names.append("URG")
        return "|".join(flag_names) if flag_names else "None"
    
    def _detect_encoding(self, data):
        """æª¢æ¸¬è³‡æ–™ç·¨ç¢¼"""
        try:
            data.decode('utf-8')
            return "UTF-8"
        except:
            try:
                data.decode('ascii')
                return "ASCII"
            except:
                return "Binary"
    
    def _generate_json_filename(self, pcap_file, mode):
        """ç”ŸæˆJSONæª”æ¡ˆå"""
        base_name = os.path.splitext(os.path.basename(pcap_file))[0]
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if mode == 'embedded_analysis':
            return f"{base_name}_embedded_{timestamp}.json"
        elif mode == 'restored_analysis':
            return f"{base_name}_restored_{timestamp}.json"
        else:
            return f"{base_name}_general_{timestamp}.json"
    
    def get_service_statistics(self):
        """ç²å–æœå‹™çµ±è¨ˆ"""
        current_time = time.time()
        start_time = self.conversion_stats['service_start_time']
        
        runtime = current_time - start_time if start_time else 0
        
        return {
            **self.conversion_stats,
            "service_runtime_seconds": runtime,
            "files_per_hour": (self.conversion_stats['total_files_processed'] / (runtime / 3600)) if runtime > 0 else 0,
            "processed_files_list": list(self.processed_files),
            "current_time": datetime.now().isoformat()
        }
    
    def shutdown(self):
        """é—œé–‰æœå‹™"""
        print("\nğŸ›‘ æ­£åœ¨é—œé–‰JSONè½‰æ›æœå‹™...")
        
        self.is_running = False
        
        # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        stats = self.get_service_statistics()
        
        print(f"\nğŸ“Š æœå‹™çµ±è¨ˆ:")
        print(f"   é‹è¡Œæ™‚é–“: {stats['service_runtime_seconds']:.1f} ç§’")
        print(f"   è™•ç†æª”æ¡ˆ: {stats['total_files_processed']} å€‹")
        print(f"   è½‰æ›å°åŒ…: {stats['total_packets_converted']} å€‹")
        print(f"   é‚„åŸå°åŒ…: {stats['total_restored_packets']} å€‹")
        print(f"   å¤±æ•—è½‰æ›: {stats['failed_conversions']} å€‹")
        print(f"   è™•ç†æ•ˆç‡: {stats['files_per_hour']:.1f} æª”æ¡ˆ/å°æ™‚")
        
        # é¡¯ç¤ºå·²è™•ç†æª”æ¡ˆ
        if self.processed_files:
            print(f"\nğŸ“ å·²è™•ç†æª”æ¡ˆ:")
            for i, file_path in enumerate(sorted(self.processed_files), 1):
                print(f"   {i}. {os.path.basename(file_path)}")
        
        # è¨ˆç®—JSONæª”æ¡ˆçµ±è¨ˆ
        try:
            import fnmatch
            json_files = []
            if os.path.exists(self.json_output_dir):
                all_files = os.listdir(self.json_output_dir)
                json_files = [os.path.join(self.json_output_dir, f) 
                             for f in all_files if fnmatch.fnmatch(f, "*.json")]
            
            total_json_size = sum(os.path.getsize(f) for f in json_files if os.path.exists(f))
            
            print(f"\nğŸ“„ JSONè¼¸å‡ºçµ±è¨ˆ:")
            print(f"   JSONæª”æ¡ˆæ•¸: {len(json_files)} å€‹")
            print(f"   ç¸½å¤§å°: {total_json_size/1024:.1f} KB")
            
        except Exception as e:
            print(f"   JSONçµ±è¨ˆè¨ˆç®—éŒ¯èª¤: {e}")
        
        print("âœ… JSONè½‰æ›æœå‹™å·²å®‰å…¨é—œé–‰")


def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸ”„ è‡ªå‹•åŒ–PCAPåˆ°JSONè½‰æ›æœå‹™")
    print("=" * 60)
    print("å¾config_surveiling_sample.jsonè®€å–è¨­å®š")
    print("è‡ªå‹•ç›£æ§PCAPç›®éŒ„ä¸¦è½‰æ›ç‚ºJSON")
    print("=" * 60)
    
    # å–å¾—é…ç½®æª”æ¡ˆè·¯å¾‘
    config_path = input("é…ç½®æª”æ¡ˆè·¯å¾‘ (é è¨­: ./config_surveiling_sample.json): ").strip()
    if not config_path:
        config_path = "./config_surveiling_sample.json"
    
    # å»ºç«‹æœå‹™å¯¦ä¾‹
    service = AutoJsonConverterService(config_path)
    
    try:
        # å•Ÿå‹•æœå‹™
        success = service.start_service()
        
        if not success:
            print("âŒ JSONè½‰æ›æœå‹™å•Ÿå‹•å¤±æ•—")
            return 1
            
    except KeyboardInterrupt:
        print("\nâš ï¸  æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿ")
    except Exception as e:
        print(f"âŒ æœå‹™é‹è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return 1
    finally:
        service.shutdown()
    
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())